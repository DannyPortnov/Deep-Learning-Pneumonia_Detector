{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1c74a3rxlanh3TTjhoNRBKUdhNPZ1ahj2","timestamp":1688995399225}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhS3l6TBIFsH","executionInfo":{"status":"ok","timestamp":1689055912359,"user_tz":-180,"elapsed":19951,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"}},"outputId":"d836d7c5-da2f-4eec-f535-f5b44b133926"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cp8qW-GCa5oP","outputId":"5b518aaf-f16d-42be-9e07-081326375494","executionInfo":{"status":"ok","timestamp":1689056122102,"user_tz":-180,"elapsed":188838,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-37c23209f9b0>:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return np.array(normal_data), np.array(bacterial_data), np.array(virus_data)\n"]}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n","from keras import regularizers\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.utils import shuffle\n","from keras.callbacks import ReduceLROnPlateau\n","import tensorflow as tf\n","import cv2\n","import os\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","from numpy.typing import NDArray\n","from typing import Optional, Literal, Type, TypedDict\n","\n","labels = ['PNEUMONIA', 'NORMAL']\n","IMG_SIZE = 150\n","\n","\n","#Concatenate all images in the different folders to three lists: normal, bacterial and virus\n","\n","def get_training_data(data_dir) -> NDArray:\n","    normal_data = []\n","    bacterial_data = []\n","    virus_data = []\n","    for label in labels:\n","        path = os.path.join(data_dir, label)\n","        class_num = labels.index(label)\n","        for img in os.listdir(path):\n","            try:\n","                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n","                # Reshaping images to preferred size\n","                resized_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n","                if label == 'NORMAL':\n","                  normal_data.append([resized_arr, class_num])\n","                elif label == 'PNEUMONIA':\n","                  if \"bacteria\" in str(img):\n","                    bacterial_data.append([resized_arr, class_num])\n","                  elif \"virus\" in str(img):\n","                    virus_data.append([resized_arr, class_num])\n","            except Exception as e:\n","                print(e)\n","    return np.array(normal_data), np.array(bacterial_data), np.array(virus_data)\n","\n","\n","# Includes both normal and pneumonia cases.\n","normal_data, bacterial_data, virus_data = get_training_data('/content/drive/MyDrive/Deep Learning Project/chest_xray/')  # list of [image, label]\n"]},{"cell_type":"code","source":["# Separate data and labels for each class\n","X_normal = [data[0] for data in normal_data]\n","y_normal = [data[1] for data in normal_data]\n","\n","X_bacterial = [data[0] for data in bacterial_data]\n","y_bacterial = [data[1] for data in bacterial_data]\n","\n","X_virus = [data[0] for data in virus_data]\n","y_virus = [data[1] for data in virus_data]\n","\n","# Perform train-test split for each class separately\n","X_normal_train, X_normal_test, y_normal_train, y_normal_test = train_test_split(X_normal, y_normal, test_size=0.15, random_state=42)\n","X_bacterial_train, X_bacterial_test, y_bacterial_train, y_bacterial_test = train_test_split(X_bacterial, y_bacterial, test_size=0.05, random_state=42)\n","X_virus_train, X_virus_test, y_virus_train, y_virus_test = train_test_split(X_virus, y_virus, test_size=0.075, random_state=42)\n","\n","# Split the training sets into training and validation sets for each class\n","X_normal_train, X_normal_val, y_normal_train, y_normal_val = train_test_split(X_normal_train, y_normal_train, test_size=0.038, random_state=42)\n","X_bacterial_train, X_bacterial_val, y_bacterial_train, y_bacterial_val = train_test_split(X_bacterial_train, y_bacterial_train, test_size=0.0095, random_state=42)\n","X_virus_train, X_virus_val, y_virus_train, y_virus_val = train_test_split(X_virus_train, y_virus_train, test_size=0.019, random_state=42)\n","\n","# Merge the data and labels for all classes\n","X_train = np.concatenate((X_normal_train, X_bacterial_train, X_virus_train), axis=0)\n","y_train = np.concatenate((y_normal_train, y_bacterial_train, y_virus_train), axis=0)\n","X_val = np.concatenate((X_normal_val, X_bacterial_val, X_virus_val), axis=0)\n","y_val = np.concatenate((y_normal_val, y_bacterial_val, y_virus_val), axis=0)\n","X_test = np.concatenate((X_normal_test, X_bacterial_test, X_virus_test), axis=0)\n","y_test = np.concatenate((y_normal_test, y_bacterial_test, y_virus_test), axis=0)\n","\n","# Shuffle the training data\n","X_train, y_train = shuffle(X_train, y_train, random_state=42)\n","\n","# Normalize data\n","X_train = X_train / 255.0\n","X_val = X_val / 255.0\n","X_test = X_test / 255.0\n","\n","# Reshape data for deep learning\n","X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","X_val = X_val.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","\n","# Rest of the code for data augmentation and model training\n","datagen = ImageDataGenerator(\n","    featurewise_center=False,\n","    samplewise_center=False,\n","    featurewise_std_normalization=False,\n","    samplewise_std_normalization=False,\n","    zca_whitening=False,\n","    rotation_range=30,\n","    zoom_range=0.2,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    horizontal_flip=True,\n","    vertical_flip=False)\n","\n","datagen.fit(X_train)\n"],"metadata":{"id":"cI80WxRU05lu","executionInfo":{"status":"ok","timestamp":1689056123036,"user_tz":-180,"elapsed":938,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# --------- Building CNN network ----------#\n","# num_of_test = 1\n","# for i in range(num_of_test):\n","kernelSize = (2,2)\n","model = Sequential()\n","model.add(Conv2D(filters=32, kernel_size=kernelSize, padding='same',\n","                activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n","model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(filters=64, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=64, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(filters=128, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=128, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(BatchNormalization())\n","model.add(Flatten())\n","model.add(Dropout(0.2))\n","\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(units=1, activation='sigmoid'))\n","\n","\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n","              loss='binary_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","# binary_crossentropy is chosen because we have a binary ouput, either sick or not.\n","\n","learning_rate_reduction = ReduceLROnPlateau(\n","    monitor='val_loss', patience=2, verbose=1, factor=0.2, min_lr=0.00000001)\n","# min_delta: threshold for measuring the new optimum, to only focus on\n","#   significant changes.\n","early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,restore_best_weights=True)\n","# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","\n","history = model.fit(X_train, y_train, batch_size=16,\n","                    epochs=30, validation_data=(X_val, y_val), callbacks=[learning_rate_reduction, early_stop])\n","# score = model.evaluate(X_test, y_test, verbose=0)\n","\n","# train_acc = history.history['accuracy']\n","# val_acc = history.history['val_accuracy']\n","# train_loss = history.history['loss']\n","# val_loss = history.history['val_loss']\n","\n","\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_accuracy)\n","\n","\n","fig, ax = plt.subplots(1, 2)\n","\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(train_acc) + 1)\n","\n","fig.set_size_inches(20, 10)\n","\n","ax[0].plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n","ax[0].plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n","ax[0].set_title('Training & Validation Accuracy')\n","ax[0].legend()\n","ax[0].set_xlabel(\"Epochs\")\n","ax[0].set_ylabel(\"Accuracy\")\n","\n","ax[1].plot(epochs, train_loss, 'b-o', label='Training Loss')\n","ax[1].plot(epochs, val_loss, 'r-o', label='Validation Loss')\n","ax[1].set_title('Training & Validation Loss')\n","ax[1].legend()\n","ax[1].set_xlabel(\"Epochs\")\n","ax[1].set_ylabel(\"Loss\")\n","plt.show()"],"metadata":{"id":"GGjANsVbNKW3"},"execution_count":null,"outputs":[]}]}