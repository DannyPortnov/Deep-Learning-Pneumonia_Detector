{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19951,"status":"ok","timestamp":1689055912359,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"GhS3l6TBIFsH","outputId":"d836d7c5-da2f-4eec-f535-f5b44b133926"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188838,"status":"ok","timestamp":1689056122102,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"cp8qW-GCa5oP","outputId":"5b518aaf-f16d-42be-9e07-081326375494"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import keras\n","import tensorflow as tf\n","import cv2\n","import os\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.utils import shuffle"]},{"cell_type":"markdown","metadata":{},"source":["Generate Data From chest_xray Folder:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = ['NORMAL', 'PNEUMONIA']  # NORMAL = 0 , PNEUMONIA = 1\n","IMG_SIZE = 150\n","\n","\n","#Concatenate all images in the different folders to three lists: normal, bacterial and virus\n","\n","def get_training_data(data_dir):\n","    normal_data = []\n","    bacterial_data = []\n","    virus_data = []\n","    for label in labels:\n","        path = os.path.join(data_dir, label)\n","        class_num = labels.index(label)\n","        for img in os.listdir(path):\n","            try:\n","                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n","                # Reshaping images to preferred size\n","                resized_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n","                if label == 'NORMAL':\n","                  normal_data.append([resized_arr, class_num])\n","                elif label == 'PNEUMONIA':\n","                  if \"bacteria\" in str(img):\n","                    bacterial_data.append([resized_arr, class_num])\n","                  elif \"virus\" in str(img):\n","                    virus_data.append([resized_arr, class_num])\n","            except Exception as e:\n","                print(e)\n","    return np.array(normal_data), np.array(bacterial_data), np.array(virus_data)\n","\n","\n","# Includes both normal and pneumonia cases.\n","normal_data, bacterial_data, virus_data = get_training_data('/content/drive/MyDrive/Deep Learning Project/chest_xray/')  # list of [image, label]\n"]},{"cell_type":"markdown","metadata":{},"source":["Generate Train, Validation and Test Sets:"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":938,"status":"ok","timestamp":1689056123036,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"cI80WxRU05lu"},"outputs":[],"source":["# Separate data and labels for each class\n","X_normal = [data[0] for data in normal_data]\n","y_normal = [data[1] for data in normal_data]\n","\n","X_bacterial = [data[0] for data in bacterial_data]\n","y_bacterial = [data[1] for data in bacterial_data]\n","\n","X_virus = [data[0] for data in virus_data]\n","y_virus = [data[1] for data in virus_data]\n","\n","# Perform train-test split for each class separately\n","X_normal_train, X_normal_test, y_normal_train, y_normal_test = train_test_split(X_normal, y_normal, test_size=0.15, random_state=42)\n","X_bacterial_train, X_bacterial_test, y_bacterial_train, y_bacterial_test = train_test_split(X_bacterial, y_bacterial, test_size=0.05, random_state=42)\n","X_virus_train, X_virus_test, y_virus_train, y_virus_test = train_test_split(X_virus, y_virus, test_size=0.075, random_state=42)\n","\n","# Split the training sets into training and validation sets for each class\n","X_normal_train, X_normal_val, y_normal_train, y_normal_val = train_test_split(X_normal_train, y_normal_train, test_size=0.038, random_state=42)\n","X_bacterial_train, X_bacterial_val, y_bacterial_train, y_bacterial_val = train_test_split(X_bacterial_train, y_bacterial_train, test_size=0.0095, random_state=42)\n","X_virus_train, X_virus_val, y_virus_train, y_virus_val = train_test_split(X_virus_train, y_virus_train, test_size=0.019, random_state=42)\n","\n","# Merge the data and labels for all classes\n","X_train = np.concatenate((X_normal_train, X_bacterial_train, X_virus_train), axis=0)\n","y_train = np.concatenate((y_normal_train, y_bacterial_train, y_virus_train), axis=0)\n","X_val = np.concatenate((X_normal_val, X_bacterial_val, X_virus_val), axis=0)\n","y_val = np.concatenate((y_normal_val, y_bacterial_val, y_virus_val), axis=0)\n","X_test = np.concatenate((X_normal_test, X_bacterial_test, X_virus_test), axis=0)\n","y_test = np.concatenate((y_normal_test, y_bacterial_test, y_virus_test), axis=0)\n","\n","# Shuffle the training data\n","X_train, y_train = shuffle(X_train, y_train, random_state=42)\n","\n","# Normalize data\n","X_train = X_train / 255.0\n","X_val = X_val / 255.0\n","X_test = X_test / 255.0\n","\n","# Reshape data for deep learning\n","X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","X_val = X_val.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","\n","# Rest of the code for data augmentation and model training\n","datagen = ImageDataGenerator(\n","    featurewise_center=False,\n","    samplewise_center=False,\n","    featurewise_std_normalization=False,\n","    samplewise_std_normalization=False,\n","    zca_whitening=False,\n","    rotation_range=30,\n","    zoom_range=0.2,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    horizontal_flip=True,\n","    vertical_flip=False)\n","\n","datagen.fit(X_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["Building CNN network:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGjANsVbNKW3"},"outputs":[],"source":["kernelSize = (3,3)\n","poolSize = (2,2)\n","strideSize = (2,2)\n","model = Sequential()\n","model.add(Conv2D(filters=32, kernel_size=kernelSize, padding='same',\n","                activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n","model.add(MaxPool2D(pool_size=poolSize,strides=strideSize))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(filters=64, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=64, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=poolSize,strides=strideSize))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(filters=128, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=128, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=poolSize,strides=strideSize))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(filters=256, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=256, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=poolSize,strides=strideSize))\n","model.add(BatchNormalization())\n","model.add(Flatten())\n","model.add(Dropout(0.2))\n","\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(units=1, activation='sigmoid'))"]},{"cell_type":"markdown","metadata":{},"source":["Compile and Summerize The Model:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","              loss='binary_crossentropy', metrics=['accuracy'])\n","# binary_crossentropy is chosen because we have a binary ouput, either sick or not.\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Train the Model:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learning_rate_reduction = ReduceLROnPlateau(\n","    monitor='val_loss', patience=4, verbose=1, factor=0.2, min_lr=0.00000001)\n","# min_delta: threshold for measuring the new optimum, to only focus on\n","#   significant changes.\n","early_stop = EarlyStopping(monitor='val_loss', patience=9,restore_best_weights=False)\n","# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","\n","history = model.fit(X_train, y_train, batch_size=16,\n","                    epochs=30, validation_data=(X_val, y_val), callbacks=[learning_rate_reduction, early_stop])\n","# score = model.evaluate(X_test, y_test, verbose=0)\n"]},{"cell_type":"markdown","metadata":{},"source":["Evaluate the Accuracy and Loss:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_accuracy)\n","\n","\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(train_acc) + 1)"]},{"cell_type":"markdown","metadata":{},"source":["Plot Graphs of the Training and Validation Accuracy and Loss vs Epochs:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_fig = plt.figure()\n","plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n","plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n","plt.title('Training & Validation Accuracy vs. Ephochs')\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","\n","loss_fig = plt.figure()\n","plt.plot(epochs, train_loss, 'b-o', label='Training Loss')\n","plt.plot(epochs, val_loss, 'r-o', label='Validation Loss')\n","plt.title('Training & Validation Loss vs. Ephochs')\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Precision vs. Recall Graph with F1-Score Points Marked on the Plot:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming you have a trained sequential model named 'model'\n","y_scores = model.predict(X_test)  # X_test is the input data for testing\n","\n","# y_true is the true binary labels\n","# y_scores is the predicted scores for each sample\n","# Define thresholds\n","thresholds = np.arange(0.1, 0.95, 0.05)\n","\n","precision = []  # Initialize an empty list for precision scores\n","recall = []  # Initialize an empty list for recall scores\n","\n","for t in thresholds:\n","    y_pred = y_scores > t\n","    p = precision_score(y_test, y_pred)\n","    r = recall_score(y_test, y_pred)\n","    precision.append(p)\n","    recall.append(r)\n","    print(f'Threshold: {t:.2f}, Precision: {p:.2f}, Recall: {r:.2f}')\n","\n","f_scores = [f1_score(y_test, y_scores > t) for t in thresholds]\n","\n","# plot the precision-recall curve\n","plt.plot(recall, precision)\n","\n","# plot the F-score points per threshold on the precision-recall curve\n","plt.scatter(recall, precision, c=f_scores, cmap='viridis')\n","plt.colorbar(label='F-score')\n","\n","# add text annotations with the F-score values\n","for r, p, f in zip(recall, precision, f_scores):\n","    plt.annotate(f'f1={f:.3f}', (r, p), xytext=(4, 2), textcoords='offset points', fontsize=6, rotation=20)\n","\n","plt.title('Precision-Recall curve')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","# Set the number of digits after the decimal point on the x-axis\n","ax = plt.gca()\n","ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n","plt.show()\n","\n","# Find the maximum F1-score and its corresponding threshold\n","max_f1_score = max(f_scores)\n","max_threshold = thresholds[f_scores.index(max_f1_score)]\n","\n","print(f\"Maximum F1-score: {max_f1_score:.3f}\")\n","print(f\"Corresponding threshold: {max_threshold:.2f}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1c74a3rxlanh3TTjhoNRBKUdhNPZ1ahj2","timestamp":1688995399225}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
