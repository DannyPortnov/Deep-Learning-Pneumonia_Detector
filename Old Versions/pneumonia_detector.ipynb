{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "cp8qW-GCa5oP",
        "outputId": "32d52351-afe4-482f-d043-78ba78aae150"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ddf89b70cc66>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Includes both normal and pneumonia cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mlungs_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chest_xray/'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# list of [image, label]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mx_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNDArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlungs_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ddf89b70cc66>\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mclass_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chest_xray/PNEUMONIA'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from numpy.typing import NDArray\n",
        "from typing import Optional, Literal, Type, TypedDict\n",
        "\n",
        "labels = ['PNEUMONIA', 'NORMAL']\n",
        "IMG_SIZE = 150\n",
        "\n",
        "\n",
        "def get_training_data(data_dir) -> NDArray:\n",
        "    data = []\n",
        "    for label in labels:\n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                # Reshaping images to preferred size\n",
        "                resized_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return np.array(data)\n",
        "\n",
        "\n",
        "# Includes both normal and pneumonia cases.\n",
        "lungs_dataset = get_training_data('chest_xray/')  # list of [image, label]\n",
        "\n",
        "x_dataset: NDArray = np.empty((len(lungs_dataset),))\n",
        "y_dataset: NDArray = np.empty((len(lungs_dataset),))\n",
        "for x, y in lungs_dataset:\n",
        "    np.insert(x_dataset, 1, x)\n",
        "    np.insert(y_dataset, 1, x)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_dataset, y_dataset, random_state=42,\n",
        "                                                    test_size=0.2, shuffle=True)\n",
        "\n",
        "# When the Random_state is not defined in the code for every run train data will\n",
        "#  change and accuracy might change for every run. When the\n",
        "#  Random_state = \" constant integer\" is defined then train data will be constant\n",
        "#  For every run so that it will make easy to debug.\n",
        "\n",
        "# On a serious note, random_state simply sets a seed to the random generator,\n",
        "#  so that your train-test splits are always deterministic.\n",
        "#  If you don't set a seed, it is different each time.\n",
        "\n",
        "# Normalize data\n",
        "X_train = np.array(X_train) / 255\n",
        "X_test = np.array(X_test) / 255\n",
        "\n",
        "# When using the image as it is and passing through a Deep Neural Network,\n",
        "#  the computation of high numeric values may become more complex.\n",
        "# To reduce this we can normalize the values to range from 0 to 1.\n",
        "\n",
        "# In this way, the numbers will be small and the computation becomes\n",
        "#  easier and faster.\n",
        "# As the pixel values range from 0 to 256, apart from 0 the range is 255.\n",
        "#  So dividing all the values by 255 will convert it to range from 0 to 1.\n",
        "\n",
        "# It is not needed anymore. The reason for normalizing the images\n",
        "#  is to avoid the possibility of exploding gradients because\n",
        "#  of the high range of the pixels [0, 255], and improve the\n",
        "#  convergence speed. Therefore, you either standardize the each image,\n",
        "#  so that the range is [-1, 1] or you just divide the with the\n",
        "#  maximum pixel value as you are doing, so that the range of the\n",
        "#  pixels is in the [0, 1] range.\n",
        "\n",
        "# Another reason why you might want to normalize the image data is\n",
        "#  if you are using transfer learning. For example, if you are using\n",
        "#  a pre-trained model that has been trained with images with pixels\n",
        "#  in the [0, 1] range, you should make sure that the inputs you are\n",
        "#  providing the model are in the same range. Otherwise, your results\n",
        "#  will be messed up.\n",
        "\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "# resize data for deep learning\n",
        "X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "\n",
        "# With data augmentation to prevent overfitting and handling the imbalance in dataset\n",
        "# Because the dataset is small we \"increase\" the dataset by change of images parameters.\n",
        "# In this way we increase our dataset and prevent overfitting.\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    zoom_range=0.2,  # Randomly zoom image\n",
        "    # randomly shift images horizontally (fraction of total width)\n",
        "    width_shift_range=0.1,\n",
        "    # randomly shift images vertically (fraction of total height)\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "\n",
        "# --------- Building CNN network ----------#\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), strides=1, padding='same',\n",
        "          activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
        "model.add(MaxPool2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPool2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPool2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# binary_crossentropy is chosen because we have a binary ouput, either sick or not.\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(\n",
        "    monitor='val_loss', patience=5, verbose=1, factor=0.2, min_lr=0.00000001, min_delta=0.001)\n",
        "# min_delta: threshold for measuring the new optimum, to only focus on\n",
        "#   significant changes.\n",
        "\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                    epochs=30, validation_split=0.025, callbacks=[learning_rate_reduction])\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"val Loss:\", score[0])\n",
        "print(\"val Accuracy:\", score[1])\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test Loss:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])\n",
        "\n",
        "epochs = [i for i in range(30)]\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "fig.set_size_inches(20, 10)\n",
        "\n",
        "ax[0].plot(epochs, train_acc, 'go-', label='Training Accuracy')\n",
        "ax[0].plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
        "ax[0].set_title('Training & Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "ax[1].plot(epochs, train_loss, 'g-o', label='Training Loss')\n",
        "ax[1].plot(epochs, val_loss, 'r-o', label='Validation Loss')\n",
        "ax[1].set_title('Testing Accuracy & Loss')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Training & Validation Loss\")\n",
        "plt.show()\n"
      ]
    }
  ]
}