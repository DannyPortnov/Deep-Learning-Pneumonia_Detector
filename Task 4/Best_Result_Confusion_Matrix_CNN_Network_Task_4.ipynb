{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17704,"status":"ok","timestamp":1689146500587,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"GhS3l6TBIFsH","outputId":"2ddde2b7-d6fc-4b66-b207-5abc9c6c261f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cp8qW-GCa5oP"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import keras\n","import tensorflow as tf\n","import cv2\n","import os\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","from keras.models import Sequential\n","from keras.optimizers import Adam, SGD, RMSprop\n","from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.utils import shuffle"]},{"cell_type":"markdown","metadata":{"id":"K69pGv3y-Sl6"},"source":["Generate Data From chest_xray Folder:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":202821,"status":"ok","timestamp":1689146714988,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"IxvI7eME-Sl8","outputId":"e5b6263a-36d6-4611-da1d-ee517f497559"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-3-80e0e9a86ce1>:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return np.array(normal_data), np.array(bacterial_data), np.array(virus_data)\n"]}],"source":["labels = ['NORMAL', 'PNEUMONIA']  # NORMAL = 0 , PNEUMONIA = 1\n","all_classes_labels = ['NORMAL', 'BACTERIA', 'VIRUS']  # NORMAL = 0 , BACTERIA = 1, VIRUS = 2\n","IMG_SIZE = 150\n","\n","\n","#Concatenate all images in the different folders to three lists: normal, bacterial and virus\n","\n","def get_training_data(data_dir):\n","    normal_data = []\n","    bacterial_data = []\n","    virus_data = []\n","    for label in labels:\n","        path = os.path.join(data_dir, label)\n","        for img in os.listdir(path):\n","            try:\n","                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n","                # Reshaping images to preferred size\n","                resized_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n","                if label == 'NORMAL':\n","                  class_num = all_classes_labels.index('NORMAL')\n","                  normal_data.append([resized_arr, class_num])\n","                elif label == 'PNEUMONIA':\n","                  if \"bacteria\" in str(img):\n","                    class_num = all_classes_labels.index('BACTERIA')\n","                    bacterial_data.append([resized_arr, class_num])\n","                  elif \"virus\" in str(img):\n","                    class_num = all_classes_labels.index('VIRUS')\n","                    virus_data.append([resized_arr, class_num])\n","            except Exception as e:\n","                print(e)\n","    return np.array(normal_data), np.array(bacterial_data), np.array(virus_data)\n","\n","\n","# Includes both normal and pneumonia cases.\n","normal_data, bacterial_data, virus_data = get_training_data('/content/drive/MyDrive/Deep Learning Project/chest_xray/')  # list of [image, label]\n"]},{"cell_type":"markdown","metadata":{"id":"qVP5T_SI-Sl9"},"source":["Generate Train, Validation and Test Sets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cI80WxRU05lu"},"outputs":[],"source":["SEED = 42\n","# Separate data and labels for each class\n","X_normal = [data[0] for data in normal_data]\n","y_normal = [data[1] for data in normal_data]\n","\n","X_bacterial = [data[0] for data in bacterial_data]\n","y_bacterial = [data[1] for data in bacterial_data]\n","\n","X_virus = [data[0] for data in virus_data]\n","y_virus = [data[1] for data in virus_data]\n","\n","# Perform train-test split for each class separately\n","X_normal_train, X_normal_test, y_normal_train, y_normal_test = train_test_split(X_normal, y_normal, test_size=0.15, random_state=SEED)\n","X_bacterial_train, X_bacterial_test, y_bacterial_train, y_bacterial_test = train_test_split(X_bacterial, y_bacterial, test_size=0.05, random_state=SEED)\n","X_virus_train, X_virus_test, y_virus_train, y_virus_test = train_test_split(X_virus, y_virus, test_size=0.075, random_state=SEED)\n","\n","# Split the training sets into training and validation sets for each class\n","X_normal_train, X_normal_val, y_normal_train, y_normal_val = train_test_split(X_normal_train, y_normal_train, test_size=0.038, random_state=SEED)\n","X_bacterial_train, X_bacterial_val, y_bacterial_train, y_bacterial_val = train_test_split(X_bacterial_train, y_bacterial_train, test_size=0.0095, random_state=SEED)\n","X_virus_train, X_virus_val, y_virus_train, y_virus_val = train_test_split(X_virus_train, y_virus_train, test_size=0.019, random_state=SEED)\n","\n","# Merge the data and labels for all classes\n","X_train = np.concatenate((X_normal_train, X_bacterial_train, X_virus_train), axis=0)\n","y_train = np.concatenate((y_normal_train, y_bacterial_train, y_virus_train), axis=0)\n","X_val = np.concatenate((X_normal_val, X_bacterial_val, X_virus_val), axis=0)\n","y_val = np.concatenate((y_normal_val, y_bacterial_val, y_virus_val), axis=0)\n","X_test = np.concatenate((X_normal_test, X_bacterial_test, X_virus_test), axis=0)\n","y_test = np.concatenate((y_normal_test, y_bacterial_test, y_virus_test), axis=0)\n","\n","# Shuffle the training data\n","X_train, y_train = shuffle(X_train, y_train, random_state=SEED)\n","\n","# Normalize data\n","X_train = X_train / 255.0\n","X_val = X_val / 255.0\n","X_test = X_test / 255.0\n","\n","# Reshape data for deep learning\n","X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","X_val = X_val.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n","\n","# Rest of the code for data augmentation and model training\n","datagen = ImageDataGenerator(\n","    featurewise_center=False,\n","    samplewise_center=False,\n","    featurewise_std_normalization=False,\n","    samplewise_std_normalization=False,\n","    zca_whitening=False,\n","    rotation_range=30,\n","    zoom_range=0.2,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=False)\n","\n","datagen.fit(X_train)\n"]},{"cell_type":"markdown","metadata":{"id":"2j8qFLUN-Sl-"},"source":["Building CNN Network:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGjANsVbNKW3"},"outputs":[],"source":["def create_model():\n","    kernelSize = (3,3)\n","    poolSize = (2,2)\n","    strideSize = (2,2)\n","    # If you are creating many models in a loop, this global state will consume \n","    # an increasing amount of memory over time, and you may want to clear it. \n","    # Calling clear_session() releases the global state: this helps avoid clutter \n","    # from old models and layers, especially when memory is limited.\n","    keras.backend.clear_session()\n","    \n","    model = Sequential()\n","    model.add(Conv2D(filters=32, kernel_size=kernelSize, padding='same',\n","                    activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n","    model.add(MaxPool2D(pool_size=poolSize,strides=strideSize))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(filters=64, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","    model.add(Conv2D(filters=64, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","    model.add(MaxPool2D(pool_size=poolSize,strides=strideSize))\n","    model.add(BatchNormalization())\n","\n","    model.add(Conv2D(filters=128, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","    model.add(Conv2D(filters=128, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","    model.add(MaxPool2D(pool_size=poolSize,strides=strideSize))\n","    model.add(BatchNormalization())\n","\n","    model.add(Conv2D(filters=256, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","    model.add(Conv2D(filters=256, kernel_size=kernelSize, padding=\"same\", activation=\"relu\"))\n","    model.add(MaxPool2D(pool_size=poolSize,strides=strideSize))\n","    model.add(BatchNormalization())\n","    model.add(Flatten())\n","    model.add(Dropout(0.2))\n","\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dense(units=3, activation='softmax'))\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"psQWpX5JwqcC"},"source":["Try Different Epochs and Learning Rates\n","* Compile and Train the Model for each Epoch and Learning Rate\n","* Plot Graphs of the Training and Validation Accuracy and Loss vs Epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3Gk8hmc-SmA"},"outputs":[],"source":["Epochs = 20\n","lr= 0.0005\n","\n","opt = Adam\n","\n","model = create_model()\n","model.compile(optimizer=opt(learning_rate=lr),\n","            loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=4,\n","                                        verbose=1, factor=0.2, min_lr=0.00000001)\n","\n","history = model.fit(X_train, y_train,\n","                    batch_size=16,\n","                    epochs=Epoch,\n","                    callbacks=[learning_rate_reduction],\n","                    validation_data=(X_val, y_val))\n","\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_accuracy)\n","\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(train_acc) + 1)\n","\n","# Create a figure and subplot\n","fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Plot the training and validation accuracy\n","axs[0].plot(train_acc, 'bo-', label='Training Accuracy')\n","axs[0].plot(val_acc, 'ro-', label='Validation Accuracy')\n","axs[0].set_title(f'Training & Validation Accuracy\\n opt={opt.__name__}, lr={lr}, Epochs={Epochs}')\n","# axs[0].title.set_size(10) # if title is too big, change the size here \n","axs[0].legend(loc='lower right')\n","axs[0].set_xlabel(\"Epochs\")\n","axs[0].set_ylabel(\"Accuracy\")\n","\n","# Plot the training and validation loss\n","axs[1].plot(train_loss, 'bo-', label='Training Loss')\n","axs[1].plot(val_loss, 'ro-', label='Validation Loss')\n","axs[1].set_title(f'Training & Validation Loss\\n opt={opt.__name__}, lr={lr}, Epochs={Epochs}')\n","# axs[1].title.set_size(10) # if title is too big, change the size here \n","axs[1].legend(loc='upper right')\n","axs[1].set_xlabel(\"Epochs\")\n","axs[1].set_ylabel(\"Loss\")\n","\n","# Adjust spacing between subplots\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred = model.predict(X_test)  # X_test is the input data for testing\n","y_pred = np.argmax(y_pred,axis = 1)\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Create confusion matrix display\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Bacteria', 'Virus'])\n","\n","disp.plot()\n","plt.title('Confusion Matrix')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1-wf5MerCD0xNFrsitrcID6YyEAms-uYy","timestamp":1689259849538},{"file_id":"1c74a3rxlanh3TTjhoNRBKUdhNPZ1ahj2","timestamp":1688995399225}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
