{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhS3l6TBIFsH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWpejyuoUk6K"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import glob\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.applications import ResNet152V2, ResNet50V2\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout, Input\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b-sB5kvehLS"
      },
      "source": [
        "Generate Data From chest_xray Folder:\n",
        "* Here we split the data in a way that we have bacteria, virus and normal images. in the proper way- some of each and not in a total randomize way\n",
        "* TODO: Need to chek why the depth of the image is 3 and not 1 as in regular cnn network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_vp5wwNehLT"
      },
      "outputs": [],
      "source": [
        "# IMG_SIZE = 150\n",
        "IMG_DEPTH = 3\n",
        "IMG_SIZE = 224\n",
        "BATCH = 16\n",
        "SEED = 42\n",
        "\n",
        "normal_dataset = glob.glob('/content/drive/MyDrive/Deep Learning Project/chest_xray/NORMAL/*.jpeg')\n",
        "pneumonia_dataset = glob.glob('/content/drive/MyDrive/Deep Learning Project/chest_xray/PNEUMONIA/*.jpeg')\n",
        "virus_dataset = list(filter(lambda x: 'virus' in x, pneumonia_dataset))\n",
        "bacterial_dataset = list(filter(lambda x: 'bacteria' in x, pneumonia_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJVGfKm_ehLU"
      },
      "source": [
        "Generate Train, Validation and Test Sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJsEuk2qehLU"
      },
      "outputs": [],
      "source": [
        "def split_data(dataSet, testSize, valSize):\n",
        "    train, test= train_test_split(dataSet, test_size=testSize, random_state=SEED, shuffle=True)\n",
        "    train, val= train_test_split(train, test_size=valSize, random_state=SEED, shuffle=True)\n",
        "    return train, test, val\n",
        "\n",
        "\n",
        "train_normal, test_normal, val_normal = split_data(normal_dataset, 0.15, 0.038)\n",
        "train_bacterial, test_bacterial, val_bacterial = split_data(bacterial_dataset, 0.05, 0.0095)\n",
        "train_virus, test_virus, val_virus = split_data(virus_dataset, 0.075, 0.019)\n",
        "\n",
        "\n",
        "\n",
        "train = [x for x in train_normal]\n",
        "train.extend([x for x in train_bacterial])\n",
        "train.extend([x for x in train_virus])\n",
        "\n",
        "test = [x for x in test_normal]\n",
        "test.extend([x for x in test_bacterial])\n",
        "test.extend([x for x in test_virus])\n",
        "\n",
        "val = [x for x in val_normal]\n",
        "val.extend([x for x in val_bacterial])\n",
        "val.extend([x for x in val_virus])\n",
        "\n",
        "df_train = pd.DataFrame(np.concatenate([['Normal']*(len(train_normal)) , ['Pneumonia']*(len(train_bacterial) + len(train_virus))]), columns = ['class'])\n",
        "df_train['image'] = [x for x in train]\n",
        "\n",
        "df_val = pd.DataFrame(np.concatenate([['Normal']*(len(val_normal)) , ['Pneumonia']*(len(val_bacterial) + len(val_virus))]), columns = ['class'])\n",
        "df_val['image'] = [x for x in val]\n",
        "\n",
        "df_test = pd.DataFrame(np.concatenate([['Normal']*len(test_normal) , ['Pneumonia']*(len(test_bacterial) + len(test_virus))]), columns = ['class'])\n",
        "df_test['image'] = [x for x in test]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9DKC2xiehLV"
      },
      "source": [
        "Perform Data Augmentation on Each Set:\n",
        "* TODO: Need to check why we are doing data augmentation on all 3 sets\n",
        "  and not just on the training set as in the regular cnn network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3rEW4eVehLV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# With data augmentation to prevent overfitting and handling the imbalance in dataset\n",
        "# Because the dataset is small we \"increase\" the dataset by change of images parameters.\n",
        "# In this way we increase our dataset and prevent overfitting.\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.,\n",
        "                                  zoom_range = 0.1,\n",
        "                                  #rotation_range = 0.1,\n",
        "                                  width_shift_range = 0.1,\n",
        "                                  height_shift_range = 0.1)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "ds_train = train_datagen.flow_from_dataframe(df_train,\n",
        "                                             #directory=train_path, #dataframe contains the full paths\n",
        "                                             x_col = 'image',\n",
        "                                             y_col = 'class',\n",
        "                                             target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                             class_mode = 'binary',\n",
        "                                             batch_size = BATCH,\n",
        "                                             seed = SEED)\n",
        "\n",
        "ds_val = test_datagen.flow_from_dataframe(df_val,\n",
        "                                            #directory=train_path,\n",
        "                                            x_col = 'image',\n",
        "                                            y_col = 'class',\n",
        "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                            class_mode = 'binary',\n",
        "                                            batch_size = BATCH,\n",
        "                                            seed = SEED)\n",
        "\n",
        "ds_test = test_datagen.flow_from_dataframe(df_test,\n",
        "                                            #directory=test_path,\n",
        "                                            x_col = 'image',\n",
        "                                            y_col = 'class',\n",
        "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                            class_mode = 'binary',\n",
        "                                            batch_size = 1,\n",
        "                                            shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd6zlpzyehLW"
      },
      "source": [
        "Create and Freeze the Base Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yozEK1sjehLW"
      },
      "outputs": [],
      "source": [
        "# --------- create base model ----------#\n",
        "base_model = ResNet50V2(\n",
        "    weights='imagenet',\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, IMG_DEPTH),\n",
        "    include_top=False)\n",
        "\n",
        "# --------- FREEZE base model ----------#\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Re0Fo9OjS6V"
      },
      "source": [
        "**TODO:** check why we need to split the validation set from the training set in advance instead of using validation split.\n",
        "\n",
        "the following comment is not working:\n",
        "we use dataaugmantion on all of the training set (including the validatio set because we use validation split in model/fit method). maybe we should check what is the difference in the results if we split the validation set before we train and use data augmentation only on the actual training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRslXFVIehLX"
      },
      "source": [
        "Get the Pre-Trained Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XoCNp0KehLX"
      },
      "outputs": [],
      "source": [
        "# --------- get pretrained model ----------#\n",
        "keras.backend.clear_session()\n",
        "\n",
        "#Input shape = [width, height, color channels]\n",
        "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, IMG_DEPTH))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# Head\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "# x = layers.Dense(64, activation='relu')(x)\n",
        "# x = layers.Dropout(0.2)(x)\n",
        "\n",
        "#Final Layer (Output)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs=[inputs], outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjFja1cRehLY"
      },
      "source": [
        "Compile and Summerize The Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK5eSDg6UnXs"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "               optimizer = Adam(learning_rate=0.001), metrics='accuracy')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9wEBQB4ehLY"
      },
      "source": [
        "Train the Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ3JElGlehLY"
      },
      "outputs": [],
      "source": [
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    min_delta=1e-7,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor = 0.2,\n",
        "    patience = 2,\n",
        "    min_delt = 1e-7,\n",
        "    cooldown = 0,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "Epochs = 20\n",
        "\n",
        "history = model.fit(ds_train,\n",
        "          batch_size=BATCH, epochs=Epochs,\n",
        "          validation_data=ds_val,\n",
        "          # callbacks=[early_stop, learning_rate_reduction],\n",
        "          steps_per_epoch=(len(df_train)/BATCH),\n",
        "          validation_steps=(len(df_val)/BATCH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PhBcP_wehLZ"
      },
      "source": [
        "Evaluate the Accuracy and Loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmbdmqcLehLZ"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(ds_test, verbose=0)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(train_acc) + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVydSUjuehLZ"
      },
      "source": [
        "Plot Graphs of the Training and Validation Accuracy and Loss vs Epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pu9KGp6ehLZ"
      },
      "outputs": [],
      "source": [
        "accuracy_fig = plt.figure()\n",
        "plt.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
        "plt.title('Training & Validation Accuracy vs. Epochs')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "loss_fig = plt.figure()\n",
        "plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
        "plt.title('Training & Validation Loss vs. Epochs')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cejrhOrUehLZ"
      },
      "source": [
        "Precision vs. Recall Graph with F1-Score Points Marked on the Plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vqiMc0uehLa"
      },
      "outputs": [],
      "source": [
        "num_label = {'Normal': 0, 'Pneumonia' : 1}\n",
        "y_test = df_test['class'].copy().map(num_label).astype('int')\n",
        "ds_test.reset()\n",
        "y_scores = model.predict(ds_test, steps=len(ds_test), verbose=0)  # X_test is the input data for testing\n",
        "\n",
        "# y_true is the true binary labels\n",
        "# y_scores is the predicted scores for each sample\n",
        "# Define thresholds\n",
        "thresholds = np.arange(0.1, 0.95, 0.05)\n",
        "\n",
        "precision = []  # Initialize an empty list for precision scores\n",
        "recall = []  # Initialize an empty list for recall scores\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred = y_scores > t\n",
        "    p = precision_score(y_test, y_pred)\n",
        "    r = recall_score(y_test, y_pred)\n",
        "    precision.append(p)\n",
        "    recall.append(r)\n",
        "    print(f'Threshold: {t:.2f}, Precision: {p:.2f}, Recall: {r:.2f}')\n",
        "\n",
        "f_scores = [f1_score(y_test, y_scores > t) for t in thresholds]\n",
        "\n",
        "# plot the precision-recall curve\n",
        "plt.plot(recall, precision)\n",
        "\n",
        "# plot the F-score points per threshold on the precision-recall curve\n",
        "plt.scatter(recall, precision, c=f_scores, cmap='viridis')\n",
        "plt.colorbar(label='F-score')\n",
        "\n",
        "# add text annotations with the F-score values\n",
        "for r, p, f in zip(recall, precision, f_scores):\n",
        "    plt.annotate(f'f1={f:.3f}', (r, p), xytext=(4, 2), textcoords='offset points', fontsize=6, rotation=20)\n",
        "\n",
        "plt.title('Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "# # Set the number of digits after the decimal point on the x-axis\n",
        "# ax = plt.gca()\n",
        "# ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
        "plt.show()\n",
        "\n",
        "# Find the maximum F1-score and its corresponding threshold\n",
        "max_f1_score = max(f_scores)\n",
        "max_threshold = thresholds[f_scores.index(max_f1_score)]\n",
        "\n",
        "print(f\"Maximum F1-score: {max_f1_score:.3f}\")\n",
        "print(f\"Corresponding threshold: {max_threshold:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}