{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17232,"status":"ok","timestamp":1689000484208,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"GhS3l6TBIFsH","outputId":"88bf30a8-c3f3-4913-9c8d-9cce4f49ee5e"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":609,"status":"ok","timestamp":1689001659713,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"yWpejyuoUk6K","outputId":"1183e993-3219-4f79-82e1-b7f4d7061677"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","import glob\n","import keras\n","import tensorflow as tf\n","from keras.applications import ResNet152V2\n","from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout, Input\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.utils import shuffle"]},{"cell_type":"markdown","metadata":{},"source":["Generate Data From chest_xray Folder:\n","* Here we split the data in a way that we have bacteria, virus and normal images. in the proper way- some of each and not in a total randomize way"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# IMG_SIZE = 150\n","IMG_DEPTH = 3\n","IMG_SIZE = 224\n","BATCH = 32\n","SEED = 42\n","\n","normal_dataset = glob.glob('/content/drive/MyDrive/Deep Learning Project/chest_xray/NORMAL/*.jpeg')\n","pneumonia_dataset = glob.glob('/content/drive/MyDrive/Deep Learning Project/chest_xray/PNEUMONIA/*.jpeg')\n","virus_dataset = list(filter(lambda x: 'virus' in x, pneumonia_dataset))\n","bacterial_dataset = list(filter(lambda x: 'bacteria' in x, pneumonia_dataset))\n"]},{"cell_type":"markdown","metadata":{},"source":["Generate Train, Validation and Test Sets:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_data(dataSet, testSize, valSize):\n","    train, test= train_test_split(dataSet, test_size=testSize, random_state=42, shuffle=True)\n","    train, val= train_test_split(train, test_size=valSize, random_state=42, shuffle=True)\n","    return train, test, val\n","\n","\n","train_normal, test_normal, val_normal = split_data(normal_dataset, 0.15, 0.038)\n","train_bacterial, test_bacterial, val_bacterial = split_data(bacterial_dataset, 0.05, 0.0095)\n","train_virus, test_virus, val_virus = split_data(virus_dataset, 0.075, 0.019)\n","\n","\n","\n","train = [x for x in train_normal]\n","train.extend([x for x in train_bacterial])\n","train.extend([x for x in train_virus])\n","\n","test = [x for x in test_normal]\n","test.extend([x for x in test_bacterial])\n","test.extend([x for x in test_virus])\n","\n","val = [x for x in val_normal]\n","val.extend([x for x in val_bacterial])\n","val.extend([x for x in val_virus])\n","\n","df_train = pd.DataFrame(np.concatenate([['Normal']*(len(train_normal)) , ['Pneumonia']*(len(train_bacterial) + len(train_virus))]), columns = ['class'])\n","df_train['image'] = [x for x in train]\n","\n","df_val = pd.DataFrame(np.concatenate([['Normal']*(len(val_normal)) , ['Pneumonia']*(len(val_bacterial) + len(val_virus))]), columns = ['class'])\n","df_val['image'] = [x for x in val]\n","\n","df_test = pd.DataFrame(np.concatenate([['Normal']*len(test_normal) , ['Pneumonia']*(len(test_bacterial) + len(test_virus))]), columns = ['class'])\n","df_test['image'] = [x for x in test]\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Perform Data Augmentation on Each Set:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# With data augmentation to prevent overfitting and handling the imbalance in dataset\n","# Because the dataset is small we \"increase\" the dataset by change of images parameters.\n","# In this way we increase our dataset and prevent overfitting.\n","\n","train_datagen = ImageDataGenerator(rescale=1/255.,\n","                                  zoom_range = 0.1,\n","                                  #rotation_range = 0.1,\n","                                  width_shift_range = 0.1,\n","                                  height_shift_range = 0.1)\n","\n","test_datagen = ImageDataGenerator(rescale=1/255.)\n","\n","ds_train = train_datagen.flow_from_dataframe(df_train,\n","                                             #directory=train_path, #dataframe contains the full paths\n","                                             x_col = 'image',\n","                                             y_col = 'class',\n","                                             target_size = (IMG_SIZE, IMG_SIZE),\n","                                             class_mode = 'binary',\n","                                             batch_size = BATCH,\n","                                             seed = SEED)\n","\n","ds_val = test_datagen.flow_from_dataframe(df_val,\n","                                            #directory=train_path,\n","                                            x_col = 'image',\n","                                            y_col = 'class',\n","                                            target_size = (IMG_SIZE, IMG_SIZE),\n","                                            class_mode = 'binary',\n","                                            batch_size = BATCH,\n","                                            seed = SEED)\n","\n","ds_test = test_datagen.flow_from_dataframe(df_test,\n","                                            #directory=test_path,\n","                                            x_col = 'image',\n","                                            y_col = 'class',\n","                                            target_size = (IMG_SIZE, IMG_SIZE),\n","                                            class_mode = 'binary',\n","                                            batch_size = 1,\n","                                            shuffle = False)"]},{"cell_type":"markdown","metadata":{},"source":["Create and Freeze the Base Model:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# --------- create base model ----------#\n","base_model = ResNet152V2(\n","    weights='imagenet',\n","    input_shape=(IMG_SIZE, IMG_SIZE, IMG_DEPTH),\n","    include_top=False)\n","\n","# --------- FREEZE base model ----------#\n","base_model.trainable = False"]},{"cell_type":"markdown","metadata":{"id":"3Re0Fo9OjS6V"},"source":["**TODO:** check why we need to split the validation set from the training set in advance instead of using validation split.\n","\n","the following comment is not working:\n","we use dataaugmantion on all of the training set (including the validatio set because we use validation split in model/fit method). maybe we should check what is the difference in the results if we split the validation set before we train and use data augmentation only on the actual training set."]},{"cell_type":"markdown","metadata":{},"source":["Get the Pre-Trained Model:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# --------- get pretrained model ----------#\n","keras.backend.clear_session()\n","\n","#Input shape = [width, height, color channels]\n","inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","\n","x = base_model(inputs)\n","\n","# Head\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.2)(x)\n","# x = layers.Dense(64, activation='relu')(x)\n","# x = layers.Dropout(0.2)(x)\n","\n","#Final Layer (Output)\n","output = Dense(1, activation='sigmoid')(x)\n","\n","model = keras.Model(inputs=[inputs], outputs=output)"]},{"cell_type":"markdown","metadata":{},"source":["Compile and Summerize The Model:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1275912,"status":"ok","timestamp":1688983050896,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"sK5eSDg6UnXs","outputId":"2e7e47b2-15f5-4726-bf21-03db7f8b4971"},"outputs":[],"source":["model.compile(loss='binary_crossentropy',\n","               optimizer = keras.optimizers.Adam(learning_rate=0.001), metrics='accuracy')\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Train the Model:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["early_stop = EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    min_delta=1e-7,\n","    restore_best_weights=True,\n",")\n","\n","learning_rate_reduction = ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor = 0.2,\n","    patience = 2,\n","    min_delt = 1e-7,\n","    cooldown = 0,\n","    verbose = 1\n",")\n","\n","history = model.fit(ds_train,\n","          batch_size = BATCH, epochs = 10,\n","          validation_data=ds_val,\n","          callbacks=[early_stop, learning_rate_reduction],\n","          steps_per_epoch=(len(df_train)/BATCH),\n","          validation_steps=(len(df_val)/BATCH))"]},{"cell_type":"markdown","metadata":{},"source":["Evaluate the Accuracy and Loss:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_loss, test_accuracy = model.evaluate(ds_test, verbose=0)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_accuracy)\n","\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","val_acc = history.history['val_accuracy']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(train_acc) + 1)"]},{"cell_type":"markdown","metadata":{},"source":["Plot Graphs of the Training and Validation Accuracy and Loss vs Epochs:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(1, 2)\n","fig.set_size_inches(20, 10)\n","\n","ax[0].plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n","ax[0].plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n","ax[0].set_title('Training & Validation Accuracy')\n","ax[0].legend()\n","ax[0].set_xlabel(\"Epochs\")\n","ax[0].set_ylabel(\"Accuracy\")\n","\n","ax[1].plot(epochs, train_loss, 'b-o', label='Training Loss')\n","ax[1].plot(epochs, val_loss, 'r-o', label='Validation Loss')\n","ax[1].set_title('Training & Validation Loss')\n","ax[1].legend()\n","ax[1].set_xlabel(\"Epochs\")\n","ax[1].set_ylabel(\"Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Precision vs. Recall Graph with F1-Score Points Marked on the Plot:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Assuming you have a trained sequential model named 'model'\n","num_label = {'Normal': 0, 'Pneumonia' : 1}\n","y_test = df_test['class'].copy().map(num_label).astype('int')\n","ds_test.reset()\n","y_scores = model.predict(ds_test, steps=len(ds_test), verbose=0)  # X_test is the input data for testing\n","\n","# y_true is the true binary labels\n","# y_scores is the predicted scores for each sample\n","# Define thresholds\n","thresholds = np.arange(0.1, 0.95, 0.05)\n","\n","precision = []  # Initialize an empty list for precision scores\n","recall = []  # Initialize an empty list for recall scores\n","\n","for t in thresholds:\n","    y_pred = y_scores > t\n","    p = precision_score(y_test, y_pred)\n","    r = recall_score(y_test, y_pred)\n","    precision.append(p)\n","    recall.append(r)\n","    print(f'Threshold: {t:.2f}, Precision: {p:.2f}, Recall: {r:.2f}')\n","\n","f_scores = [f1_score(y_test, y_scores > t) for t in thresholds]\n","\n","# plot the precision-recall curve\n","plt.plot(recall, precision, label='Precision-Recall curve')\n","\n","# plot the F-score points per threshold on the precision-recall curve\n","plt.scatter(recall, precision, c=f_scores, cmap='viridis')\n","plt.colorbar(label='F-score')\n","\n","# add text annotations with the F-score values\n","for r, p, f in zip(recall, precision, f_scores):\n","    plt.annotate(f'f1={f:.3f}', (r, p), xytext=(5, 5), textcoords='offset points', fontsize=6, rotation=25)\n","\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.legend()\n","plt.show()\n","\n","# Find the maximum F1-score and its corresponding threshold\n","max_f1_score = max(f_scores)\n","max_threshold = thresholds[f_scores.index(max_f1_score)]\n","\n","print(f\"Maximum F1-score: {max_f1_score:.3f}\")\n","print(f\"Corresponding threshold: {max_threshold:.2f}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNMZP0uxFknxJLohy35iuZJ","gpuType":"T4","provenance":[{"file_id":"14HhG9GX6tFUrCYnmgHEcyHJmEzmkXHIl","timestamp":1688998702749}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
