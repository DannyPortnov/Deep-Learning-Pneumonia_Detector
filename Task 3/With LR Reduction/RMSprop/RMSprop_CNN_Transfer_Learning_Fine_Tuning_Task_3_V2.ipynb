{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17232,"status":"ok","timestamp":1689000484208,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"GhS3l6TBIFsH","outputId":"88bf30a8-c3f3-4913-9c8d-9cce4f49ee5e"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":609,"status":"ok","timestamp":1689001659713,"user":{"displayName":"Niv Kopolovitch","userId":"02253133704912854642"},"user_tz":-180},"id":"yWpejyuoUk6K","outputId":"1183e993-3219-4f79-82e1-b7f4d7061677"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","import glob\n","import keras\n","import tensorflow as tf\n","from keras.applications import ResNet152V2, ResNet50V2\n","from keras.optimizers import Adam, SGD, RMSprop\n","from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout, Input\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.utils import shuffle"]},{"cell_type":"markdown","metadata":{},"source":["Generate Data From chest_xray Folder:\n","* Here we split the data in a way that we have bacteria, virus and normal images. in the proper way- some of each and not in a total randomize way\n","* TODO: Need to chek why the depth of the image is 3 and not 1 as in regular cnn network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# IMG_SIZE = 150\n","IMG_DEPTH = 3\n","IMG_SIZE = 224\n","BATCH = 16\n","SEED = 42\n","\n","normal_dataset = glob.glob('/content/drive/MyDrive/Deep Learning Project/chest_xray/NORMAL/*.jpeg')\n","pneumonia_dataset = glob.glob('/content/drive/MyDrive/Deep Learning Project/chest_xray/PNEUMONIA/*.jpeg')\n","virus_dataset = list(filter(lambda x: 'virus' in x, pneumonia_dataset))\n","bacterial_dataset = list(filter(lambda x: 'bacteria' in x, pneumonia_dataset))\n"]},{"cell_type":"markdown","metadata":{},"source":["Generate Train, Validation and Test Sets:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_data(dataSet, testSize, valSize):\n","    train, test= train_test_split(dataSet, test_size=testSize, random_state=SEED, shuffle=True)\n","    train, val= train_test_split(train, test_size=valSize, random_state=SEED, shuffle=True)\n","    return train, test, val\n","\n","\n","train_normal, test_normal, val_normal = split_data(normal_dataset, 0.15, 0.038)\n","train_bacterial, test_bacterial, val_bacterial = split_data(bacterial_dataset, 0.05, 0.0095)\n","train_virus, test_virus, val_virus = split_data(virus_dataset, 0.075, 0.019)\n","\n","\n","\n","train = [x for x in train_normal]\n","train.extend([x for x in train_bacterial])\n","train.extend([x for x in train_virus])\n","\n","test = [x for x in test_normal]\n","test.extend([x for x in test_bacterial])\n","test.extend([x for x in test_virus])\n","\n","val = [x for x in val_normal]\n","val.extend([x for x in val_bacterial])\n","val.extend([x for x in val_virus])\n","\n","df_train = pd.DataFrame(np.concatenate([['Normal']*(len(train_normal)) , ['Pneumonia']*(len(train_bacterial) + len(train_virus))]), columns = ['class'])\n","df_train['image'] = [x for x in train]\n","\n","df_val = pd.DataFrame(np.concatenate([['Normal']*(len(val_normal)) , ['Pneumonia']*(len(val_bacterial) + len(val_virus))]), columns = ['class'])\n","df_val['image'] = [x for x in val]\n","\n","df_test = pd.DataFrame(np.concatenate([['Normal']*len(test_normal) , ['Pneumonia']*(len(test_bacterial) + len(test_virus))]), columns = ['class'])\n","df_test['image'] = [x for x in test]\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Perform Data Augmentation on Each Set:\n","* TODO: Need to check why we are doing data augmentation on all 3 sets \n","  and not just on the training set as in the regular cnn network."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# With data augmentation to prevent overfitting and handling the imbalance in dataset\n","# Because the dataset is small we \"increase\" the dataset by change of images parameters.\n","# In this way we increase our dataset and prevent overfitting.\n","\n","train_datagen = ImageDataGenerator(rescale=1/255.,\n","                                  zoom_range = 0.1,\n","                                  #rotation_range = 0.1,\n","                                  width_shift_range = 0.1,\n","                                  height_shift_range = 0.1)\n","\n","test_datagen = ImageDataGenerator(rescale=1/255.)\n","\n","ds_train = train_datagen.flow_from_dataframe(df_train,\n","                                             #directory=train_path, #dataframe contains the full paths\n","                                             x_col = 'image',\n","                                             y_col = 'class',\n","                                             target_size = (IMG_SIZE, IMG_SIZE),\n","                                             class_mode = 'binary',\n","                                             batch_size = BATCH,\n","                                             seed = SEED)\n","\n","ds_val = test_datagen.flow_from_dataframe(df_val,\n","                                            #directory=train_path,\n","                                            x_col = 'image',\n","                                            y_col = 'class',\n","                                            target_size = (IMG_SIZE, IMG_SIZE),\n","                                            class_mode = 'binary',\n","                                            batch_size = BATCH,\n","                                            seed = SEED)\n","\n","ds_test = test_datagen.flow_from_dataframe(df_test,\n","                                            #directory=test_path,\n","                                            x_col = 'image',\n","                                            y_col = 'class',\n","                                            target_size = (IMG_SIZE, IMG_SIZE),\n","                                            class_mode = 'binary',\n","                                            batch_size = 1,\n","                                            shuffle = False)"]},{"cell_type":"markdown","metadata":{},"source":["# Building CNN Network- Transfer Learning with Fine Tuning"]},{"cell_type":"markdown","metadata":{},"source":["Create and Freeze the Base Model and Get the Pre-Trained Model\n","* This function creates a base model and freezeing all of its layers."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_base_model():\n","    # --------- create base model ----------#\n","    base_model = ResNet50V2(\n","        weights='imagenet',\n","        input_shape=(IMG_SIZE, IMG_SIZE, IMG_DEPTH),\n","        include_top=False)\n","\n","    # --------- FREEZE base model ----------#\n","    base_model.trainable = False\n","    return base_model\n"]},{"cell_type":"markdown","metadata":{},"source":["Get the Pre-Trained Model\n","* If you are creating many models in a loop, this global state will consume \n","  an increasing amount of memory over time, and you may want to clear it. \n","  Calling clear_session() releases the global state: this helps avoid clutter \n","  from old models and layers, especially when memory is limited.\n","* We use: keras.backend.clear_session()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_pre_trained_model(base_model):\n","    # --------- get pretrained model ----------#\n","    keras.backend.clear_session()\n","\n","    #Input shape = [width, height, color channels]\n","    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, IMG_DEPTH))\n","\n","    x = base_model(inputs, training=False)\n","\n","    # Head\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dense(128, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","\n","    #Final Layer (Output)\n","    output = Dense(1, activation='sigmoid')(x)\n","\n","    pre_trained_model = keras.Model(inputs=[inputs], outputs=output)\n","    \n","    return pre_trained_model"]},{"cell_type":"markdown","metadata":{},"source":["Adding Fine Tuning\n","* This function un-freezes the top layers of the base model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_fine_tuning(base_model):\n","    # Set the base_model as trainable\n","    base_model.trainable = True\n","\n","    # Print the number of layers in the base_model\n","    print(f\"Number of Layers In the Base Model: {len(base_model.layers)}\")\n","\n","    # Specify the layer index from which fine-tuning will start\n","    fine_tune_from = -70\n","\n","    # Iterate through the layers of the base_model\n","    # and set them as non-trainable up to the fine_tune_from index\n","    for layer in base_model.layers[:fine_tune_from]:\n","        layer.trainable = False"]},{"cell_type":"markdown","metadata":{},"source":["Try Different Epochs and Learning Rates\n","* Compile and Train the Model for each Epoch and Learning Rate.\n","\n","* Plot Graphs of the Training and Validation Accuracy and Loss vs Epochs.\n","\n","* It is critical to only do fine tuning after the model with frozen layers has been trained to convergence. \n","  If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, \n","  the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\n","\n","* It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Epochs = [10, 20, 40]\n","learning_rates= [0.001, 0.0005, 0.0001]\n","\n","opt = RMSprop\n","\n","learning_rate_reduction = ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor = 0.2,\n","    patience = 2,\n","    min_delt = 1e-10,\n","    cooldown = 0,\n","    verbose = 1\n",")\n","\n","lr_epoch_test_acc = {}\n","lr_epoch_test_loss = {}\n","for Epoch in Epochs:\n","  initial_epochs = Epoch//2\n","  for lr in learning_rates:\n","    base_model = create_base_model()\n","    model = get_pre_trained_model(base_model)\n","    # Training the model with all layers frozen  \n","    model.compile(loss='binary_crossentropy',\n","                optimizer = opt(learning_rate=lr), metrics='accuracy')\n","    # model.summary()\n","    history = model.fit(ds_train,\n","              batch_size=BATCH, epochs=initial_epochs,\n","              validation_data=ds_val,\n","              callbacks=[learning_rate_reduction],\n","              steps_per_epoch=(len(df_train)/BATCH),\n","              validation_steps=(len(df_val)/BATCH))\n","    # Evaluate the Accuracy and Loss (Before Fine Tuning)\n","    train_acc = history.history['accuracy']\n","    train_loss = history.history['loss']\n","    val_acc = history.history['val_accuracy']\n","    val_loss = history.history['val_loss']\n","\n","    # adding fine tuning\n","    add_fine_tuning(base_model)\n","    model.compile(loss='binary_crossentropy',\n","                optimizer = opt(learning_rate=lr/10), metrics='accuracy')\n","    history_fine = model.fit(ds_train,\n","                        batch_size=BATCH, \n","                        epochs=Epoch,\n","                        initial_epoch=history.epoch[-1],\n","                        validation_data=ds_val,\n","                        callbacks=[learning_rate_reduction],\n","                        steps_per_epoch=(len(df_train)/BATCH),\n","                        validation_steps=(len(df_val)/BATCH))\n","    # Evaluate the Accuracy and Loss (After Fine Tuning):\n","\n","    test_loss, test_accuracy = model.evaluate(ds_test, verbose=0)\n","    lr_epoch_tuple = (Epoch, lr)\n","    lr_epoch_test_acc[lr_epoch_tuple] = test_accuracy\n","    lr_epoch_test_loss[lr_epoch_tuple] = test_loss\n","    print(\"Test Loss:\", test_loss)\n","    print(\"Test Accuracy:\", test_accuracy)\n","\n","    train_acc += history_fine.history['accuracy']\n","    val_acc += history_fine.history['val_accuracy']\n","    train_loss += history_fine.history['loss']\n","    val_loss += history_fine.history['val_loss']\n","\n","    # Create a figure and subplot\n","    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n","    # Plot the training and validation accuracy\n","    axs[0].plot(train_acc, 'bo-', label='Training Accuracy')\n","    axs[0].plot(val_acc, 'ro-', label='Validation Accuracy')\n","    axs[0].set_title(f'Training & Validation Accuracy\\n opt={opt.__name__}, lr={lr}, Epochs={Epoch}')\n","    # axs[0].title.set_size(10) # if title is too big, change the size here \n","    axs[0].legend(loc='lower right')\n","    axs[0].set_xlabel(\"Epochs\")\n","    axs[0].set_ylabel(\"Accuracy\")\n","\n","    # Plot the training and validation loss\n","    axs[1].plot(train_loss, 'bo-', label='Training Loss')\n","    axs[1].plot(val_loss, 'ro-', label='Validation Loss')\n","    axs[1].set_title(f'Training & Validation Loss\\n opt={opt.__name__}, lr={lr}, Epochs={Epoch}')\n","    # axs[1].title.set_size(10) # if title is too big, change the size here \n","    axs[1].legend(loc='upper right')\n","    axs[1].set_xlabel(\"Epochs\")\n","    axs[1].set_ylabel(\"Loss\")\n","\n","    # Adjust spacing between subplots\n","    plt.subplots_adjust(wspace=0.2)\n","\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["Print the Loss & Accuracy Dictionaries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Test Accuracy and Loss Results with Optimizer: {opt.__name__}\")\n","print()\n","print()\n","def print_results_dict(Accuracy, Loss):\n","    print(\"Print Accuracy Results\")\n","    for key, val in Accuracy.items():\n","        epochs, lr = key\n","        print(f\"Epochs Number = {epochs}, Learning Rate = {lr}, Accuracy = {val:.3f}\")\n","    print()\n","    print()\n","    print(\"Print Loss Results\")\n","    for key, val in Loss.items():\n","        epochs, lr = key\n","        print(f\"Epochs Number = {epochs}, Learning Rate = {lr}, Loss = {val:.3f}\")\n","\n","\n","print_results_dict(lr_epoch_test_acc, lr_epoch_test_loss)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNMZP0uxFknxJLohy35iuZJ","gpuType":"T4","provenance":[{"file_id":"14HhG9GX6tFUrCYnmgHEcyHJmEzmkXHIl","timestamp":1688998702749}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
